# Whisper モデル比較ガイド

本ドキュメントでは、OpenAI が提供する Whisper モデルの各バリエーションの性能、速度、およびリソース消費量を比較し、本プロジェクトにおける最適な選択肢を提示します。

## モデル比較表

| モデル | パラメータ数 | VRAM 使用量 | 相対速度 | 日本語精度 (WER) |
| :--- | :---: | :---: | :---: | :---: |
| tiny | 39 M | ~1 GB | 32x | 低 |
| base | 74 M | ~1 GB | 16x | 低 |
| small | 244 M | ~2 GB | 6x | 中 |
| medium | 769 M | ~5 GB | 2x | 高 |
| large-v2 | 1.55 B | ~10 GB | 1x | 非常に高 |
| large-v3 | 1.55 B | ~10 GB | 1x | 最高 |
| **large-v3-turbo** | 809 M | **~6 GB** | **8x+** | **最高 (v3相当)** |

> [!TIP]
> **large-v3-turbo** は、`large-v3` をベースにデコーダーレイヤーを 32 から 4 に削減したモデルです。精度をほぼ維持しつつ、速度が劇的に向上し、VRAM 消費量も抑えられています。

## 各モデルの詳細と推奨用途

### 1. large-v3-turbo (推奨)
- **特徴**: 2024年10月に公開された最新の高速化モデル。
- **メリット**: `large-v3` と同等の精度を持ちながら、推論速度が 8 倍以上速い。
- **デメリット**: 翻訳タスクのデータが含まれていないため、多言語翻訳には不向き。
- **用途**: 本プロジェクト（VRChat 録音の文字起こし）における**デフォルトの推奨モデル**。精度と速度のバランスが最も優れています。

### 2. large-v3
- **特徴**: Whisper のフルサイズモデル。
- **メリット**: 最高の精度と多言語対応。
- **デメリット**: 推論が重く、VRAM を大量に消費する (10GB+)。
- **用途**: 速度よりも極限の精度を求める場合や、高性能な GPU (RTX 3090/4090等) を使用する場合。

### 3. medium / small
- **特徴**: 中規模モデル。
- **メリット**: VRAM 8GB 以下のノート PC やミドルレンジ GPU でも高速に動作する。
- **用途**: GPU リソースが限られている環境での代替案。

## faster-whisper による最適化
本プロジェクトでは、標準の `openai-whisper` ではなく `faster-whisper` を使用しています。

- **CT2 (CTranslate2)** エンジンにより、計算効率が向上。
- **Quantization (量子化)**: `int8` や `float16` を使用することで、精度を維持しながらメモリを 2〜4 倍節約可能。
- `large-v3-turbo` を `int8` で実行した場合、VRAM 消費は **2〜3GB** 程度まで抑えることが可能です。

## 本プロジェクトの設定方法
`data/config.yaml` で以下の設定を行うことで、モデルを切り替えられます。

```yaml
whisper:
  model_size: "large-v3-turbo" # large-v3, medium 等に変更可能
  device: "cuda"               # GPU を使用する場合
  compute_type: "float16"      # int8 にするとメモリ節約
```

---
> [!NOTE]
> このドキュメントは Context7 (MCP) を通じた最新の技術情報に基づいて作成されました。
